# Introduction

Cell-type annotation is a core step in single-cell RNA sequencing (RNA-seq) pipelines. The quality of annotations directly impacts downstream analyses, including mapping cellular diversity across tissues and deciphering cell-type-specific regulatory mechanisms. Manual annotation remains time-consuming and dependent on domain-specific expertise, but the rapid adoption of single-cell RNA-seq as a standard laboratory technique has created an urgent need for automated, scalable solutions¹. With repositories such as the Human Cell Atlas² and CELLxGENE³ now containing over 100 million cells, accurate and robust annotation methods are a critical first step in translating these large-scale datasets into actionable biological insights⁴.

Automated atlas-level cell-type annotation can be framed as a supervised classification problem, where models assign labels to individual cells on the basis of gene expression profiles, using reference annotations provided by original studies<sup>6–8</sup>. A defining feature of this task is that cell types are organized within a hierarchical ontology<sup>9,10</sup>, forming a multilevel taxonomy. For example, 'leukocytes' represent a broad category that contains 'lymphocytes', which in turn includes more specific subtypes such as 'B cells'. However, annotation practices vary substantially between studies—some assign broad categories, while others distinguish fine-grained subtypes. This inconsistency in label granularity introduces ambiguity into the training signal, as models must infer the appropriate level of resolution without explicit guidance.

More formally, the annotation task can be viewed as learning a function  $f: X \to Y$ , where X is the space of gene expression profiles and Y is a structured label space defined by a directed acyclic graph (DAG). In this graph, each node corresponds to a cell type and directed edges represent subtype relationships—for example, 'B cell' and 'T cell' are children of 'lymphocyte'. This structure captures relationships across varying levels of annotation granularity  $^{11-13}$ .

Many methods have been developed to perform automated cell-type annotation, ranging from logistic regression to deep learning architectures <sup>14-17</sup>. Recent benchmarking studies have shown that deep learning models outperform simpler methods as the number of cells in a dataset increases <sup>11</sup>. Importantly, these evaluations were conducted using donor-partitioned training and test splits, a design we refer to as the in-distribution (ID) setting (Fig. 1a). While useful for controlled comparisons, such splits do not reflect how cell atlases evolve in practice, where new studies are continually added and must be annotated upon release.