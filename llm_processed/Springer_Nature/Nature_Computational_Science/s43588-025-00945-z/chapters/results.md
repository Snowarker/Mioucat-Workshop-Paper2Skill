# Results

## Out-of-Distribution Performance Challenges

To better evaluate generalization to newly released studies, we consider an out-of-distribution (OOD) set-up in which models are tested on datasets not seen during training (Fig. 1b). We trained three methods with increasingly complex architectures (a linear classifier, a multilayer perceptron (MLP) and TabNet[18]) on an atlas of 15.2 million human cells annotated with 164 unique cell types, curated in the scTab study[11] from the May 2023 release of the CELLxGENE census (Fig. 1c). We then evaluate each method on 2.6 million human cells from 21 studies newly added during the 2023-12-15 release, spanning 470 donors, 16 tissues and 80 of the original 164 cell types represented in the training set. Despite being evaluated on the same cell types profiled with the same assays, macro-averaged *F*1 (macro *F*1) scores dropped by 24−32% for the linear classifier, MLP and TabNet when moving from the ID case (Fig. 1d) to the OOD setting (Fig. 1e), underscoring the limitations of current modeling strategies in generalizing across studies.

## Hierarchical Cross-Entropy Loss

To address these shortcomings, we introduce a hierarchical cross-entropy (HCE) loss that explicitly incorporates the structural relationships between cell types (Methods). Unlike standard cross-entropy (CE), which treats all classes as flat and independent, HCE enforces a consistency constraint: the probability assigned to a general cell type (for example, T cell) must be at least as high as the sum of more granular subtypes (for example, 'α–β T cell' and 'γ–δ T cell'). This prevents the model from needing to choose between broad and granular labels, since predicting a child inherently implies selecting the parent in the hierarchy (Fig. 2a).

While there are methods that leverage ontological information for cell-type annotation, they do not enforce hierarchical consistency as an integral part of their predictive framework. For example, OnClass maps both transcriptomic profiles and cell ontology structure into a joint embedding space, enabling both the annotation of unseen cell types and the identification of marker genes[12]. However, it operates primarily as a nearest-neighbor or embedding search algorithm and does not couple hierarchical relationships to the learned probabilities for each cell. As a result, sibling classes or intermediate states can still be misassigned if their embeddings overlap in feature space. As another example, popV aggregates predictions from multiple classifiers using ontology-based voting, producing robust consensus labels and uncertainty estimates for ambiguous or outlier populations[7]. However, the ontology is used only as a scaffold for post hoc reconciliation and not as a guide for model optimization. This means that hierarchical constraints are not encoded in training and possible conflicts or inconsistencies in the ensemble are resolved heuristically. In contrast, SCimilarity focuses on metric learning for scalable, cross-study retrieval of transcriptionally similar cells, using the ontology at training time to exclude ambiguous annotation pairs when sampling triplets for a contrastive loss function[19]. The learned representation supports high-quality search and transfer tasks but is not directly optimized for hierarchical or taxonomic consistency when determining class probabilities. In summary, unlike these approaches, our HCE loss explicitly encodes hierarchical dependences and relationships into the model's objective, ensuring that all predictions respect the structure of the cell ontology.

## Performance Improvements with HCE Loss

Applying the HCE loss improved OOD macro *F*1 scores by 12−15% for the linear classifier, MLP and TabNet, without modifying their architecture or tuning any hyperparameters (Fig. 2b). These consistent gains demonstrate the widespread benefits of hierarchy-aware training for cell-type annotation tasks. The HCE loss function enables the recovery of roughly half of the performance drop observed when models are applied to new studies, underscoring the practical value of aligning training objectives with ontology structure. To further assess the consistency of this effect, we evaluated performance across each of the 21 held-out studies individually (Supplementary Fig. 1). Outside of just one study while using the linear model, all HCE-trained models showed statistically significant improvements, highlighting the robustness of this approach across diverse experimental settings.

To better understand where the improvements from hierarchical training arise, we identified cell types that exhibited statistically significant changes in performance between models trained with and without the HCE loss. These differences were determined using a paired *t*-test across training runs, with *P* values corrected using the Holm–Bonferroni method (Methods). In the MLP model, for example, HCE led to improvements of up to 0.9 in *F*1 score for cell types such as 'glutamatergic neuron' and 'CD14<sup>+</sup> monocyte' (Fig. 2c). Examining these effects in relation to the cell ontology, we found that the largest gains occurred for internal nodes, particularly those embedded in densely connected regions of the DAG where related types were annotated in the training data (Fig. 2d and Supplementary Figs. 2–4). In contrast, leaf nodes—especially structurally isolated ones—showed more modest gains (Supplementary Figs. 5 and 6). This aligns with the intuition that the hierarchical loss is most effective when it can propagate signal across nearby cell types. Similar trends were observed for the linear and transformer-based models, highlighting the architecture-agnostic nature of the effect (Supplementary Fig. 7). Importantly, gains were largely unaffected by a cell type's rarity, the number of contributing studies or tissues and the diversity of sequencing technologies used—further underscoring the robustness of the approach (Supplementary Fig. 8). Finally, these gains extend to cells observed in new contexts, including across diseases and tissues not seen in the training set, where we also observe consistent improvements (Supplementary Figs. 9 and 10).